---
title: "Homework 7 R markdown"
author: "(your name here)"
date: '`r Sys.Date()`'
output:
  html_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
  word_document:
    fig_height: 4
    fig_width: 4.5
---


```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 

# Some customization.  You can alter or delete as desired (if you know what you are doing).
trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

#### <span style="color:Blue">**Intellectual Property:**</span>  
These problems are the intellectual property of the instructors and may not be reproduced outside of this course.

#### <span style="color:Crimson">**Due Date:**</span>  
Tuesday, October 24, 2017 at 11:59 PM 

***  
***  

##########################################################################
## Problem 1: Analyze Variables with Decision Trees
##########################################################################

In this problem, you will use decision trees to analyze the variables associated with which brand of orange juice customers choose to purchase.

Data Set: Load the OJ data set, which is in the ISLR package.

#####################################
### Question 1 (1 point) 

After loading the OJ data set, set the random seed equal to 7, and take a random sample of 800 rows of the data. This will be the training data set; the remaining observations will be the validation set.

Enter your R code below.

<span style="color:green">**Code Answer**: </span>
```{r, echo=TRUE}
library(ISLR)
data(OJ)
summary(OJ)
set.seed(7)

train = sample(1:dim(OJ)[1],800,replace=F)


#install.packages("tree")
library(tree)
mytree = tree(Purchase~., data = OJ[train,])
summary(mytree)
```


#####################################
### Question 2 (2 points) 

Fit a tree to the training data, using Purchase as the response variable and all other variables as predictors.  Which variables were used in constructing the tree?  Select all that apply.

<span style="color:green">**Multiple SELECT Answer** </span>
  **<span style="color:red">(AUTOGRADED on D2L)</span>**:  

	
->LoyalCH

	
Store

	
->PriceDiff

	
->SalePriceMM

	
PriceCH


#####################################
### Question 3 (2 points) 

What is the error rate on the training set? 0.1788

<span style="color:green">**Numeric Answer**  </span> 
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
```{r,echo=FALSE}
```


***

#####################################
### Question 4 (4 points)

Plot the decision tree with category labels.  Submit your plot to the Homework 7: Decision tree discussion board.  With your plot, write 3-5 sentences describing the model.

[You don't need to enter anything here.  Make sure you submit your plot and description to the discussion board.]

<span style="color:green">**Graph Answer**  </span>: 
  (post to discussion board on D2L)
```{r,echo=FALSE}
plot(mytree)
text(mytree, pretty=0)
```

Customer brand loyalty for Citrus Hill (LoyalCH) is the largest division between the groups. In fact if LoyalCH < 0.482389, then Minute Maid will be chosen. If LoyalCH >= 0.482389, then in all but one case will someone not purchase Citrus Hill. The one case is where the sale price of Minute Maid is more than 16.5 cents lower than the sale price of Citrus Hill. 

#####################################
### Question 5 (2 points)

Compute the confusion matrix for the validation set.  What is the validation set error rate?

<span style="color:green">**Numeric Answer**  </span> 
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
```{r,echo=FALSE}
item.pred = predict(mytree, OJ[-train,], type="class")
#confusion matrix
table(item.pred, OJ$Purchase[-train])
(18+21)/(18+21+147+84)


#use 10-fold CV to choose the optimal number of leaves
oj.cv = cv.tree(mytree, FUN = prune.misclass)
oj.cv
```




#####################################
### Question 6 (2 points)

Use 10-fold cross-validation on the training data to choose the number of leaves that minimizes the classification error rate.  What are the optimal numbers of leaves? Select all that apply.

<span style="color:green">**Multiple SELECT Answer** </span>
  **<span style="color:red">(AUTOGRADED on D2L)</span>**:  
1
	
2

3

4

->5

6

7

->8


#####################################
### Question 7 (1 point) 

Create a pruned tree with 5 leaves.  What is the error rate of the pruned tree on the validation set?

<span style="color:green">**Numeric Answer**  </span> 
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
```{r,echo=FALSE}
prune.oj = prune.misclass(mytree, best = 5)
prune.oj

item.pred = predict(prune.oj, OJ[-train,], type="class")
plot(prune.oj)
text(prune.oj, pretty=0)
#confusion matrix
table(item.pred, OJ$Purchase[-train])
(18+21)/(18+21+147+84)

#it looks like same error rate
```


***

##########################################################################
## Problem 2: Use Boosting to Predict Salaries
##########################################################################

In this problem, you will use boosting to predict the salaries of baseball players.

Data Set: Load the Hitters data set; it's in the ISLR package.

#####################################
### Question 8 (2 points) 

After loading the Hitters data set, remove the observations with unknown salary, and then log-transform the salaries.  

Enter your R code below.

<span style="color:green">**Code Answer**: </span>
```{r, echo=TRUE}
#https://jangorecki.gitlab.io/data.cube/library/data.table/html/na.omit.data.table.html
library(ISLR)
data("Hitters")
Hitters = na.omit(Hitters, cols="Salary")
Hitters$Salary = log(Hitters$Salary)


hist(Hitters$Salary)
summary(Hitters)

#perform boosting
#install.packages("gbm")
library(gbm)
boost = gbm(Salary~., data = Hitters, distribution = "gaussian", n.trees = 5000, shrinkage = .001, interaction.depth = 4)
summary(boost)
```

#####################################
### Question 9 (1 point) 

Perform boosting to predict log(Salary) in terms of the other variables in the data set (excluding Salary).  Use:

* 5000 trees,  
* a shrinkage parameter of .001, and  
* an interaction depth of 4.  

Which of the variables is most important for predicting log(Salary) in this model?

<span style="color:green">**Multiple choice Answer** </span>
  **<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  

	
->CAtBat

HmRun

CRuns

Years

#####################################
### Question 10 (4 points) 

Set the random seed to 7 and perform 10-fold cross-validation to compare boosting (using the same parameters as in the previous question) to multiple linear regression. 

Enter your R code below.


<span style="color:green">**Code Answer**: </span>
```{r, echo=TRUE}

#use 10 fold cross validation to estimate the error rate
n=263
k=10 #using 10-fold cross-validation
groups=c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k)) #produces list of group labels
#groups

set.seed(7)
cvgroups = sample(groups,n)
boost.predict=rep(-1,n)
lm.predict=rep(-1,n)

for(i in 1:k) {
  groupi = (cvgroups == i)
  #perform boosting and predict values of groupi
  boost = gbm(Salary~., data = Hitters[!groupi,], distribution = "gaussian", n.trees = 5000, shrinkage = .001, interaction.depth = 4)
  boost.predict[groupi] = predict(boost, newdata = Hitters[groupi,], n.trees = 5000, type="response")
  #fit linear model
  lmfitCV = lm(Salary~.,data=Hitters,subset=!groupi)
  lm.predict[groupi] = predict.lm(lmfitCV,Hitters[groupi,])
}

CV10.MSE.LM = sum((lm.predict-Hitters$Salary)^2)/n
CV10.MSE.LM

CV10.MSE.BOOSTING = sum((boost.predict-Hitters$Salary)^2)/n
CV10.MSE.BOOSTING
```


***

What MSE do you find for each method?

#####################################
### Question 11 (1 point) 

Boosting:0.2203441


<span style="color:green">**Numeric Answer**  </span> 
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
```{r,echo=FALSE}
```


#####################################
### Question 12 (1 point) 

Linear regression:0.4319816


<span style="color:green">**Numeric Answer**  </span> 
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
```{r,echo=FALSE}
```


#####################################
### Question 13 (1 point) 

Which model has the lower cross-validation MSE for this data set?

<span style="color:green">**Multiple choice Answer** </span>
  **<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  

->Boosting

Linear regression


***


##########################################################################
## Problem  3: Analyzing Salaries Through Bagging and Random Forests
##########################################################################

In this problem, you will continue your analysis of the salaries of baseball players using bagging and random forests.

Data Set: Continue to use the Hitters data set.

#####################################
### Question 14 (2 points)

Use $\texttt{?Hitters}$ to view what each variable in the data set represents.  Examine some preliminary graphs and summaries of the data.  If we want to use decision trees to analyze this data set, why are random forests a good idea? Explain in 2-4 sentences.

There are strong correlations between the predictors. Predictors like Hits and Runs for example are highly correlated. The more hits a batter gets the more runs they will have.

```{r,echo=FALSE}
pairs(Hitters)
```

<span style="color:green">**Text Answer**: </span>


#####################################
### Question 15 (1 point)
#meallen rewatch video
Perform bagging to predict log(Salary) in terms of the other variables in the data set (excluding Salary).  

Enter your R code below.

<span style="color:green">**Code Answer**: </span>
```{r, echo=TRUE}
#install.packages("randomForest")
library(randomForest)
hitters.bag = randomForest(Salary~., data=Hitters,mtry=19,importance=T)

importance(hitters.bag)
hitters.bag #% Var explained: 76.22

lmfit = lm(Salary~.,data=Hitters)
summary(lmfit)
```

#####################################
### Question 16 (1 point) 

Examining the bagged model you created in the previous question, how does the proportion of variation explained by the bagged model compare with the proportion of variation explained by multiple linear regression?

The proportion of variation explained by multiple linear regression is 55.86%, and the proportion of variation explained by the bagged model is 76.22%. The bagged model does better explaining the variation compared to the linear model.

<span style="color:green">**Text Answer**: </span>


#####################################
### Question 17 (1 point) 
Which variable is more important for predicting log(Salary) in the bagged model?

<span style="color:green">**Multiple choice Answer** </span>
  **<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  
	
->CRBI

Years

#####################################
### Question 18 (2 points)

Set the random seed to 7 again and use 10-fold cross-validation to compare bagging with random forests using 6 variables.  Write 1-2 sentences comparing the MSE of each of these methods to the MSE of boosting found in Problem 2.

The MSE for bagging is 0.03195395 and the MSE for random forests using 6 variables is 0.03184546. These are very similar. They are both significantly better than the MSE for boosting which was 0.2203441.

0.2203441

<span style="color:green">**Text Answer**: </span>
```{r, echo=TRUE}
n=263
k=10 #using 10-fold cross-validation
groups=c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k)) #produces list of group labels
#groups

set.seed(7)
cvgroups = sample(groups,n)
bag.predict=rep(-1,n)

random_forest.predict=rep(-1,n)

for(i in 1:k) {
  groupi = (cvgroups == i)
  #perform boosting and predict values of groupi
  bag = randomForest(Salary~., data=Hitters,mtry=19,importance=T)
  bag.predict[groupi] = predict(bag, newdata = Hitters[groupi,])
  
  random_forest = randomForest(Salary~., data=Hitters,mtry=6,importance=T)
  random_forest.predict[groupi] = predict(random_forest, newdata = Hitters[groupi,])
}

CV10.MSE.BAG = sum((bag.predict-Hitters$Salary)^2)/n
CV10.MSE.BAG #=0.03195395

CV10.MSE.RANDOM_FOREST = sum((random_forest.predict-Hitters$Salary)^2)/n
CV10.MSE.RANDOM_FOREST #=0.03184546
```
